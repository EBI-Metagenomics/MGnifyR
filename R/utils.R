################################### TESTING ###################################
# Methods for testing

.is_a_bool <- function(x){
    is.logical(x) && length(x) == 1L && !is.na(x)
}

.is_non_empty_character <- function(x){
    is.character(x) && all(nzchar(x))
}

.is_non_empty_string <- function(x){
    is.character(x) && length(x) == 1L
}

.is_an_integer <- function(x){
    is.numeric(x) && length(x) == 1L && x%%1==0
}

################################ HELP FUNCTIONS ################################
# Help functions that are utilized by multiple methods

########################## .mgnify_attr_list_to_df_row #########################
# Not exporting this - if people want to they can use the
# rjsonapi functionality. Internally, it takes the "attributes" list
# and converts it into a single row data.frame. For some entries, there is a
# sublist of key/value pairs. metadata_key allows these to be included as
# columns in the result.
.mgnify_attr_list_to_df_row <- function (json, metadata_key = NULL){
    # Get what kind of metadata the data includes
    attrlist <- names(json$attributes)
    # If the type of metadata is specified
    if (!is.null(metadata_key)){
        # Get metadata related to specific key
        metaattrlist <- json$attributes[[metadata_key]]
        metlist <- sapply(metaattrlist, function(x) x$value)
        names(metlist) <- sapply(metaattrlist, function(x) x$key)
        # Get metadata without the key
        baseattrlist <- attrlist[!(attrlist %in% c(metadata_key))]
        # Combine metadata
        df <- as.data.frame(t(unlist(c(
            json$attributes[baseattrlist], metlist))),
            stringsAsFactors = FALSE)
    }else{
        # Get all the metadata without key extraction
        df <- as.data.frame(t(unlist(json["attributes"])),
                            stringsAsFactors = FALSE)
    }
    # Add accession code and type of data
    df$accession <- json$id
    df$acc_type <- json$type
    # Add accession code also to rownames
    rownames(df) <- df$accession
    return(df)
}

############################## .mgnify_get_x_for_y #############################
# Helper function for getting relative paths in the API
# Not everything is implemented here - just what we
# need to get to the download or run areas
# Given an accession x, we want to get the link to get the url for the
# corresponding typeY JSONAPI path for child elements
#
# .mgnify_get_x_for_y determines the location of typeY child objects of x (typeX)
#
# This helper function, principally intended to be used internally,
# is used to match up related objects within the path. The inherently
# unhierarchical nature of the MGnify API makes it a bit inconsistent. This
# function acts as a quick way to determine how to get from one type to another,
# without having to special case within the code.
#
# Parameters:
# client MGnifyR client API object
# x Accession ID \code{char} of parent object
# typeX Type of accession \code{x}
# typeY Type of child object to return
# use.cache Whether to use on-disk cache
#
# Return:
# char complete url to access the result. Note this query is not run from here -
# just the URL is returned
#
# Examples:
# cl <- new("MgnifyClient")
# .mgnify_get_x_for_y(cl, "MGYS00005126", "studies", "samples")
.mgnify_get_x_for_y <- function(client, x, typeX, typeY, use.cache = FALSE){
    # TODO: remove comments? what is the meaning of these?
    # Are they improvements that do not work yeat or just old code?

    #This one's easy - just rearrange the URLs
    #if(typeX=="samples" & typeY %in% c("runs","studies")){
    #    paste( typeX,x,typeY, sep="/")
    #}else if(typeX=="runs" & typeY == "analyses"){
    #    paste( typeX,x,typeY, sep="/")
    #}
    #else{
    #Do it the hard way with a callout
    json_dat <- .mgnify_retrieve_json(client,
                                     paste(typeX, x, sep = "/"),
                                     use.cache = use.cache)
    #cat(str(json_dat))
    #tgt_access = json_dat[[1]]$relationships[[typeY]]$data$id
    #tgt_type = json_dat[[1]]$relationships[[typeY]]$data$type
    #paste(tgt_type,tgt_access,sep="/")
    res <- json_dat[[1]]$relationships[[typeY]]$links$related
    #substr(tgt_url, nchar(client@url) + 1, nchar(tgt_url))
    #}
    return(res)
}

############################## .mgnify_get_x_for_y #############################
# Internal function to actually perform the http request. Build up the URL then
# issues a GET, parsing the returned JSON into a nested list (uses jsonlite
# internally?) Previously cached results may be retrieved from disk without
# resorting to calling the MGnify server.

# Low level MGnify API handler
#
# .mgnify_retrieve_json deals with handles the actual HTTP GET calls for the
# MGnifyR package, handling API pagination, local result caching, and
# authentication cookies for access to restricted or pre-release datasets.
# Although principally intended for internal MGnifyR use, it's exported for
# direct invocation. Generally though it's not recommended for use by users.
#
# Parameters:
# client MGnifyR client
# path top level search point for the query. One of biomes, samples, runs etc.
# Basically includes all parts of the URL between the base API url and the
# parameter specifications
# complete_url complete url to search, usually retrieved from previous query in
# the "related" section.
# qopts named list or vector containing options/filters to be URL encoded and
# appended to query as key/value pairs
# max.hits Maximum number of data entries to return. The actual number of hits
# returned may be higher than this value, as this parameter only clamps after
# each full page is processed. Set to <=0 to disable - i.e. retrieve all items.
# use.cache Should successful queries be cached on disk locally? There are
# unresolved questions about whether this is a sensible thing to do, but it
# remains as an option. It probably makes sense for single accession grabs,
# but not for (filtered) queries - which are liable to change as new data is
# added to MGnify. Also caching only works for the first page.
# Debug Should we print out lots of information while doing the grabbing?
#
# Return:
# list of results after pagination is dealt with.

#' @importFrom urltools parameters parameters<-
#' @importFrom httr add_headers
#' @importFrom httr GET
#' @importFrom httr config
#' @importFrom httr content
.mgnify_retrieve_json <- function(
        client, path = "biomes", complete_url = NULL, qopts = NULL,
        max.hits = 200, use.cache = FALSE, Debug=FALSE){
    #client@warnings turns on debugging too:
    if(client@warnings){
        Debug <- TRUE
    }
    # Set up the base url
    # Are we using internal paths?
    if (is.null(complete_url)){
        fullurl <- paste(client@url, path, sep="/")
    }
    # Or direct links from e.g. a "related" section
    else{
        # Set the full url, but clean off any existing parameters
        # (page, format etc) as they'll be added back later:
        fullurl <- complete_url
        parameters(fullurl) <- NULL
        path <- substr(fullurl, nchar(client@url) + 2, nchar(fullurl))
    }

    #cat(fullurl)

    # Convert to csv if filters are lists.
    # This doesn't check if they  can  be searched for in the API,
    # which is an issue since no error is returned by the JSON if the search
    # is invalid - we only get a result as if no query was present...
    tmpqopts <- lapply(qopts,function(x) paste(x,collapse = ','))

    # Include the json and page position options
    # full_qopts <- as.list(c(format="json", tmpqopts, page=1))
    full_qopts <- as.list(c(format="json", tmpqopts))
    # Build up the cache name anyway - even if it's not ultimately used:
    fname_list <- c(path, names(unlist(full_qopts)), unlist(full_qopts))
    cache_fname <- paste(fname_list,collapse = "_")
    cache_full_fname <- paste(client@cacheDir, '/', cache_fname, '.RDS', sep="")


    ## Quick check to see if we should clear the disk cache  for this
    # specific call  - used for debugging and when MGnify breaks
    if(use.cache & client@clearCache){
        message(paste("clear_cache is TRUE: deleting ", cache_full_fname, sep=""))
        tryCatch(unlink(cache_full_fname), error=warning)
    }

    # Do we want to try and use a cache to speed things up?
    if(use.cache & file.exists(cache_full_fname)){
        final_data <- readRDS(cache_full_fname)
    }else{
        #Authorization: Bearer <your_token>
        if(!is.null(client@authTok)){
            add_headers(
                .headers = c(Authorization = paste("Bearer",
                                                   client@authTok, sep=" ")))
        }
        res <- GET(url=fullurl, config(verbose=Debug), query=full_qopts )
        data <- content(res)

        # Check if the search was successful
        if( res$status_code == 400 ){
            warning(data$errors[[1]]$detail, call. = FALSE)
            return(NULL)
        }
        
        if( isEmpty(data$data) ) {
            warning("Nothing was found. Make sure that ", names(full_qopts)[[2]],
                    " is correct."," Returning empty query.", call. = FALSE)
        }

        # At this point, data$data is either a list of lists or a single named
        # list. If it's a single entry, it needs embedding in a list for
        # consistency downstream datlist is built up as a list of pages, where
        # each entry must be another list. Thus, on the first page,
        #
        datlist <- list()
        if (!is.null(names(data$data))){
            #Create something to store the returned data

            datlist[[1]] <- list(data$data)
        }else{
            datlist[[1]] <- data$data
        }
        #cat(str(data))
        # Check to see if there's pagination required
        if ("meta" %in% names(data)){
            #Yes, paginate
            pstart <- as.numeric(data$meta$pagination$page)
            pend <- as.numeric(data$meta$pagination$pages)

            for (p in seq(pstart+1,pend)){    # We've already got the first one

                full_qopts$page <- p
                if(!is.null(client@authTok)){
                    add_headers(
                        .headers = c(
                            Authorization = paste("Bearer",
                                                  client@authTok, sep=" ")))
                }
                curd <- content(GET(fullurl, config(verbose=Debug),
                                    query=full_qopts ))
                datlist[[p]] <- curd$data
                #Check to see if we've pulled enough entries
                if(max.hits > 0){
                    curlen <- sum(sapply(datlist, length))
                    if (curlen > max.hits){
                        break
                    }
                }
            }
        }
        #if(length(datlist) > 1){
        final_data <- unlist(datlist, recursive=F)

        if (use.cache && !file.exists(cache_full_fname)){
            #Make sure the directory is created...
            dir.create(dirname(cache_full_fname), recursive = TRUE,
                       showWarnings = client@warnings)
            saveRDS(final_data, file = cache_full_fname)
        }
    }
    return(final_data)
}

#Internal functions to parse the attributes/hierarchy list into a data.frame
.mgnify_parse_tax <- function(json){
    df <- as.data.frame(
        c(json$attributes["count"], unlist(json$attributes$hierarchy)),
        stringsAsFactors = FALSE)
    df$index_id <- json$attributes$lineage
    df

}
.mgnify_parse_func <- function(json){
    df <- as.data.frame(json$attributes, stringsAsFactors = FALSE)
    df$index_id <- json$attributes$accession
    df
}

############################ UTILS FOR UNIT TESTING ############################
# Get different wrong arguments to test. Give right arguments as input.
# List elements are variables and their values are argument values.
.wrong_arguments <- function(var){
    # Create a mg object
    mg <- MgnifyClient()
    # List of possible arguments
    possible_params <- list(
        mg, c(mg, mg), NULL, TRUE, FALSE, c(TRUE, FALSE), 0, 1, 16, -4, 0.5,
        1.7, "test", "studies", c("studies", "assembly"), "TreeSE",
        c("TreeSE", "phyloseq"), "taxonomy-ssu", c("taxonomy-ssu", "go-slim"))

    # Loop through input variables
    res <- list()
    names <- list()
    for( x in names(var) ){
        # Get right arguments
        temp <- var[[x]]
        # Get wrong arguments
        temp <- possible_params[!possible_params %in% temp]
        # Add to lists
        res <- append(res, temp)
        names <- append(names, rep(x, length(temp)))
    }
    # Create as many columns as there are different variables. Each column
    # contains all arguments. nrow == length(all arguments)
    res <- do.call(cbind, rep(list(res), length(var)))
    colnames(res) <- names(var)
    # Add names --> which argument is for which variable?
    res <- do.call(cbind, list(names, res))
    # Loop through all variables / columns. All columns include currently same
    # info / all arguments. Those rows that are not for specific variable are
    # converted with right value of certain variable.
    for( x in names(var) ){
        temp <- var[[x]][1]
        res[res[, 1] != x, x] <- temp
    }
    # Take only variables
    res <- res[, colnames(res) %in% names(var)]
    return(res)
}
